{"databook": {"id": "databook", "block": {"id": 1000, "prop": {"html": [{"_id": "5f54d75b114c6d176d7e9765", "html": "Heading", "imageUrl": "", "tag": "h1"}, {"_id": "new_id0.46783377838438867", "html": "", "tag": "p"}, {"_id": "new_id0.5717069427903481", "html": "", "prop": {"html": [{"_id": "new_id0.2436928656749422", "prop": {"code": "# imports\r\nimport sys\r\nsys.path.append(r'\\\\fs02\\Engineering\\IndiaTeam\\Projects\\vvora\\sm')\r\nfrom data_stack import sql, datasets, report, email_smtp, chart\r\nfrom enovix_sm import fn\r\nquery=\"\"\"\r\nselect OS.FinishDate_PST, sum(os.out) as Out, sum(os.scrap) as scrap, sum(os.unscrap) as Unscrap, os.wip_type, rte.seq, os.step, concat(rte.seq,'.',os.step) as SeqStep, \r\ncase \r\nwhen os.step <> '' then '01.Test'\r\nelse '01.Test'\r\nend as Zone,\r\nos.product\r\n\r\nfrom (\r\nselect *\r\nfrom (\r\n\r\n-- 1st table - all outs from lot step\r\nselect *\r\nfrom (\r\nSelect\r\nls.lot_id,\r\nls.step,\r\nls.step as cause_step, \r\nls.upd_by,\r\nls.qty_out as Out,\r\n0 as Scrap,\r\n0 as Unscrap,\r\n'' as Code, \r\nls.time_out as FinishTime,\r\ncast (ls.time_out at time zone 'UTC' at time zone 'Pacific Standard Time' as date) FinishDate_PST,\r\nh.wip_type,\r\nh.product,\r\nrank() over (partition by h.lot_id, ls.time_out order by h.mod_ts desc) r_no\r\nfrom Lot_step ls\r\ninner join (select * from rte_step where rte_name in ('Cell_CLD_pilot')) rte on ls.step = rte.step\r\ninner join lot_header_hist h on ls.lot_id = h.lot_id and h.mod_ts < ls.time_out\r\n--where h.product like '202866%'\r\n  --and ls.step in ('STK_EPIN', 'STK_C2A', 'DECLAMP','CNSTR_WELD','BB_WELD','BB_BEND','POUCH_PROTECT','OCV0_IN','F1_BUF_BASE','F1_BUF_RM','DEGAS','OCV1_AFTERDEGAS','OCV1_AFTERCYCLING','BERGIL_CELL_PREP','OQC','30_PCT_DIM_MEAS','MRB')\r\n  and ls.tool is not null\r\n  and ls.qty_in = 1\r\n  and ls.qty_out = 1\r\n  --and ls.time_out > getdate() - 30\r\n) a where r_no=1 \r\n\r\nunion\r\n\r\n-- 2nd table - scraps\r\n\r\nselect *\r\nfrom (\r\nSelect\r\nhist.lot_id,\r\nhist.step,\r\nhist.cause_step as cause_step,\r\nhist.upd_by,\r\n0 as Out,\r\nhist.qty as Scrap,\r\n0 as Unscrap,\r\nhist.code,\r\nhist.mod_ts as FinishTime,\r\ncast (hist.mod_ts at time zone 'UTC' at time zone 'Pacific Standard Time' as date) FinishDate_PST,\r\nh.wip_type,\r\nh.product,\r\nrank() over (partition by h.lot_id, hist.mod_ts order by h.mod_ts desc) r_no\r\nfrom lot_history hist\r\ninner join (select * from rte_step where rte_name in ('Cell_CLD_pilot')) rte on hist.step = rte.step\r\ninner join lot_header_hist h on hist.lot_id = h.lot_id and h.mod_ts < hist.mod_ts\r\n--where h.product like '202866%'\r\n--  and hist.step <> 'STK_STRT'\r\n--  and hist.cause_step <> 'STK_STRT'\r\n  and hist.type in ('scrap')\r\n  --and hist.mod_ts > getdate() - 30\r\n) a where r_no=1 --and a.cause_step = 'STK_STRT' and a.FinishDate_PST > getdate() - 28 and a.wip_type = 'production'\r\n\r\nunion\r\n\r\n-- 3nd table - unscraps\r\nselect *\r\nfrom (\r\nSelect\r\nhist.lot_id,\r\nhist.step,\r\nhist.cause_step as cause_step,\r\nhist.upd_by,\r\n0 as Out,\r\n0 as Scrap,\r\nhist.qty as Unscrap,\r\nhist.code,\r\nhist.mod_ts as FinishTime,\r\ncast (hist.mod_ts at time zone 'UTC' at time zone 'Pacific Standard Time' as date) FinishDate_PST,\r\nh.wip_type,\r\nh.product,\r\nrank() over (partition by h.lot_id, hist.mod_ts order by h.mod_ts desc) r_no\r\nfrom lot_history hist\r\ninner join (select * from rte_step where rte_name in ('Cell_CLD_pilot')) rte on hist.step = rte.step\r\ninner join lot_header_hist h on hist.lot_id = h.lot_id and h.mod_ts < hist.mod_ts\r\n--where h.product like '202866%'\r\n--  and hist.step <> 'STK_STRT'\r\n--  and hist.cause_step <> 'STK_STRT'\r\n  and hist.type in ('unscrap')\r\n  -- and hist.mod_ts > getdate() - 30\r\n) a where r_no=1\r\n) os ) os\r\nleft join (select * from rte_step where rte_name in ('Cell_CLD_pilot')) rte on rte.step = os.step\r\nwhere os.FinishDate_PST > getdate() - 90\r\ngroup by FinishDate_PST, rte.seq, os.step, os.wip_type, os.product\r\n\r\n\"\"\"\r\ndf = sql.run_query(query, 'db21','cts_prod')\r\ndf"}, "type": "code", "wid": "some_id"}, {"_id": "new_id0.327954628634882", "html": "new_block8//", "prop": {"code": "df"}, "tag": "p", "type": "code"}]}, "tag": "p", "title": "expander", "type": "expander", "wid": "databook"}, {"_id": "new_id0.09707517484912453", "html": "//", "prop": {"html": [{"_id": "new_id0.5774895203387467", "prop": {"code": "def update_area_map_from_clipboard():\r\n    pd.read_clipboard().to_json('area_map1.json', orient='records')\r\ndef load_area_map():\r\n#     return pd.read_json('area_map.json').rename(columns={\"step\": \"Step\"})\r\n    return pd.read_json(r'\\\\fs02\\Engineering\\IndiaTeam\\Projects\\vvora\\sm\\data\\notebooks\\area_map.json').rename(columns={\"step\": \"Step\"})\r\n\r\ndef get_week_list(from_week, how_many):\r\n    days = str(int(from_week.split('-')[1])*7)\r\n    year = from_week.split('-')[0]\r\n    last_4_week = [(datetime.strptime(year + \"-\" + days, \"%Y-%j\") - timedelta(weeks=i)).isocalendar() for i in range(how_many)]\r\n    return [str(i[0]) + '-' +str(i[1]) for i in last_4_week]\r\n\r\ndef get_ra_df(raw_df, rolling_week = 4):\r\n    cum_df = []\r\n    for week in raw_df['WW'].dropna().unique():\r\n        days = str(int(week.split('-')[1])*7)\r\n        year = week.split('-')[0]\r\n        last_4_week = [(datetime.strptime(year + \"-\" + days, \"%Y-%j\") - timedelta(weeks=i)).isocalendar() for i in range(rolling_week)]\r\n        last_4_week = [str(i[0]) + '-' +str(i[1]) for i in last_4_week]\r\n        print(last_4_week)\r\n        if all(item in raw_df['WW'].dropna().unique() for item in last_4_week):\r\n            w_df = raw_df[raw_df['WW'].isin(last_4_week)].copy()\r\n            w_df = w_df[['SeqStep','seq','Area','product','Out','scrap','Unscrap']].groupby(['SeqStep','seq','Area','product']).sum().reset_index()\r\n            w_df['WW'] = week\r\n            w_df['ra_week'] = ','.join(last_4_week)\r\n            cum_df.append(w_df)\r\n        else:\r\n            print( ','.join(last_4_week), 'not all found in', ','.join(raw_df['WW'].dropna().unique() ))\r\n    return pd.concat(cum_df)\r\n\r\ndef calculate_yield(df):\r\n    df['yield'] = df['Out']/(df['Out'] -df['scrap'] - df['Unscrap'])\r\n    return df"}, "type": "code", "wid": "some_id"}]}, "tag": "p", "title": "expander", "type": "expander", "wid": "databook"}, {"_id": "new_id0.41377799853492436", "html": "/", "prop": {"html": [{"_id": "new_id0.7495161187192436", "prop": {"code": "available_wip_type = ['DEV','ENGG','NPI','pilot-baseline','pilot-CIP','pilot-NPI','pilot-RND','pilot-Sales','PRODUCTION','RND','SALES',]\r\nwip_type = ['NPI', 'pilot-baseline','pilot-NPI','pilot-Sales','Sales']\r\n\r\n# test filter\r\nproduct ='MN-07'\r\narea= '05.T1'\r\n"}, "type": "code", "wid": "some_id"}, {"_id": "new_id0.8934403791276222", "html": "new_bl/ock", "prop": {"code": "import plotly.express as px\r\nfig = px.bar(x=[\"a\", \"b\", \"c\"], y=[1, 3, 2])\r\n# fig.show(renderer=\"iframe\")\r\nprint('abc')\r\n\r\nfig"}, "tag": "p", "type": "code"}]}, "tag": "p", "title": "expander", "type": "expander", "wid": "databook"}, {"_id": "new_id0.3210449785706897", "html": "/c", "prop": {"code": ""}, "tag": "p", "type": "code"}, {"_id": "new_id0.06558781710469397", "html": "", "tag": "p"}, {"_id": "new_id0.945179235679654", "html": "", "tag": "p"}, {"_id": "new_id0.02928232090992955", "html": "new_b/codlock1", "prop": {"code": ""}, "tag": "p", "type": "code"}]}, "type": "editable_html", "wid": "databook", "payload": {"block": {"_id": "new_id0.5774895203387467", "prop": {"code": "def update_area_map_from_clipboard():\r\n    pd.read_clipboard().to_json('area_map1.json', orient='records')\r\ndef load_area_map():\r\n#     return pd.read_json('area_map.json').rename(columns={\"step\": \"Step\"})\r\n    return pd.read_json(r'\\\\fs02\\Engineering\\IndiaTeam\\Projects\\vvora\\sm\\data\\notebooks\\area_map.json').rename(columns={\"step\": \"Step\"})\r\n\r\ndef get_week_list(from_week, how_many):\r\n    days = str(int(from_week.split('-')[1])*7)\r\n    year = from_week.split('-')[0]\r\n    last_4_week = [(datetime.strptime(year + \"-\" + days, \"%Y-%j\") - timedelta(weeks=i)).isocalendar() for i in range(how_many)]\r\n    return [str(i[0]) + '-' +str(i[1]) for i in last_4_week]\r\n\r\ndef get_ra_df(raw_df, rolling_week = 4):\r\n    cum_df = []\r\n    for week in raw_df['WW'].dropna().unique():\r\n        days = str(int(week.split('-')[1])*7)\r\n        year = week.split('-')[0]\r\n        last_4_week = [(datetime.strptime(year + \"-\" + days, \"%Y-%j\") - timedelta(weeks=i)).isocalendar() for i in range(rolling_week)]\r\n        last_4_week = [str(i[0]) + '-' +str(i[1]) for i in last_4_week]\r\n        print(last_4_week)\r\n        if all(item in raw_df['WW'].dropna().unique() for item in last_4_week):\r\n            w_df = raw_df[raw_df['WW'].isin(last_4_week)].copy()\r\n            w_df = w_df[['SeqStep','seq','Area','product','Out','scrap','Unscrap']].groupby(['SeqStep','seq','Area','product']).sum().reset_index()\r\n            w_df['WW'] = week\r\n            w_df['ra_week'] = ','.join(last_4_week)\r\n            cum_df.append(w_df)\r\n        else:\r\n            print( ','.join(last_4_week), 'not all found in', ','.join(raw_df['WW'].dropna().unique() ))\r\n    return pd.concat(cum_df)\r\n\r\ndef calculate_yield(df):\r\n    df['yield'] = df['Out']/(df['Out'] -df['scrap'] - df['Unscrap'])\r\n    return df"}, "type": "code", "wid": "some_id"}, "parent": {"_id": "new_id0.09707517484912453", "html": "//", "prop": {"html": [{"_id": "new_id0.5774895203387467", "prop": {"code": "def update_area_map_from_clipboard():\r\n    pd.read_clipboard().to_json('area_map1.json', orient='records')\r\ndef load_area_map():\r\n#     return pd.read_json('area_map.json').rename(columns={\"step\": \"Step\"})\r\n    return pd.read_json(r'\\\\fs02\\Engineering\\IndiaTeam\\Projects\\vvora\\sm\\data\\notebooks\\area_map.json').rename(columns={\"step\": \"Step\"})\r\n\r\ndef get_week_list(from_week, how_many):\r\n    days = str(int(from_week.split('-')[1])*7)\r\n    year = from_week.split('-')[0]\r\n    last_4_week = [(datetime.strptime(year + \"-\" + days, \"%Y-%j\") - timedelta(weeks=i)).isocalendar() for i in range(how_many)]\r\n    return [str(i[0]) + '-' +str(i[1]) for i in last_4_week]\r\n\r\ndef get_ra_df(raw_df, rolling_week = 4):\r\n    cum_df = []\r\n    for week in raw_df['WW'].dropna().unique():\r\n        days = str(int(week.split('-')[1])*7)\r\n        year = week.split('-')[0]\r\n        last_4_week = [(datetime.strptime(year + \"-\" + days, \"%Y-%j\") - timedelta(weeks=i)).isocalendar() for i in range(rolling_week)]\r\n        last_4_week = [str(i[0]) + '-' +str(i[1]) for i in last_4_week]\r\n        print(last_4_week)\r\n        if all(item in raw_df['WW'].dropna().unique() for item in last_4_week):\r\n            w_df = raw_df[raw_df['WW'].isin(last_4_week)].copy()\r\n            w_df = w_df[['SeqStep','seq','Area','product','Out','scrap','Unscrap']].groupby(['SeqStep','seq','Area','product']).sum().reset_index()\r\n            w_df['WW'] = week\r\n            w_df['ra_week'] = ','.join(last_4_week)\r\n            cum_df.append(w_df)\r\n        else:\r\n            print( ','.join(last_4_week), 'not all found in', ','.join(raw_df['WW'].dropna().unique() ))\r\n    return pd.concat(cum_df)\r\n\r\ndef calculate_yield(df):\r\n    df['yield'] = df['Out']/(df['Out'] -df['scrap'] - df['Unscrap'])\r\n    return df"}, "type": "code", "wid": "some_id"}]}, "tag": "p", "title": "expander", "type": "expander", "wid": "databook"}}, "session_id": "default"}}}